# üß¨3Ô∏è‚É£ Cooperative Matrices
This is mostly for people interested in deep learning or programming GPUs to the fullest. This should also include
people interested in graphics due to DLSS, denoisers and upscaling becoming so prevalent. Cooperative matrices are
known as tensor cores on Nvidia systems.

Anyways, a GPU is a complex beast, but this section will specifically talk about one area of the chip that was
introduced into the mainstream consumer cards starting with the RTX 20-series. Ironically, the GPU started off as
quite specialized for graphics workloads, as in drawing triangles fast, but was opened up and made increasingly
flexible. With the introduction of tensor cores and ray tracing cores it somehow became both more flexible and more
specialized at the same time. I won't get into the ray tracing cores in this section, but it's pretty awesome!

Cooperative matrices are basically ALU's made for linear operations on small tiles of matrices.
They are really good at multiplying a small matrix by another small matrix and adding another small matrix, while
accumulating the result. It can do it in a number of different levels of precision.

<figure markdown>
![Image](../figures/tensor_cores_math.png){ width="500" }
<figcaption>
Nvidia's visualization of the mathematical operation a tensor core can perform.
<a href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">
Image credit </a>
</figcaption>
</figure>

While the tensor cores support matrix-matrix multiplication, they are much more limited in the size of the
multiplication. For a general linear operation, you might need to still uses loops, but you would then
be tiling your matrix instead, sending a 4x4 tile at a time of your matrices and keep track of your precisions,
such as accumulating in a higher level of precision. You can read more about it
[here](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/) and for Vulkan
[here](https://developer.nvidia.com/blog/machine-learning-acceleration-vulkan-cooperative-matrices/).

If you keep the calculations numerically stable you can even keep all of your weights during the training of
a neural network in 8-bit floating point, while accumulating in 16-bit floating point or greater,
which will greatly reduce the bandwidth needed for training. For inference, it can also yield a big
speedup all the way down to 8-bit integers. Remember, that integers compress better than
floating point numbers. So if you do quantization of your entire network before inference, you can get faster
inference and lower power consumption. The cooperative matrix was first available in CUDA, but has since been made
available in Vulkan, although the usage of it is not that wide spread yet as it is a fairly recent addition.

<figure markdown>
![Image](../figures/tensor_cores.png){ width="500" }
<figcaption>
Nvidia's visualization of a cooperative matrix.
<a href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">
Image credit </a>
</figcaption>
</figure>

## 5Ô∏è‚É£ Additional Reading
[Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)  
[Programming Tensor Cores](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)  
Vulkan
[Cooperative Matrix](https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_NV_cooperative_matrix.html)  
Machine Learning
[in Vulkan](https://developer.nvidia.com/blog/machine-learning-acceleration-vulkan-cooperative-matrices/)  
Accelerating Inference
[with Sparsity](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)  
A series of videos regarding how to use tensor cores for [mixed precision training](https://developer.nvidia.com/blog/video-mixed-precision-techniques-tensor-cores-deep-learning/)
.
